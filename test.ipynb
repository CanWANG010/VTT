{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "亲爱的雇用经理,我写信表达我对180个市场实习职位的兴趣。作为目前墨尔本大学的信息技术硕士生,在人工智能领域具有专业专长,我渴望为贵受尊敬的组织贡献数据分析和技术技能。我的教育背景和亲身体验使我成为这一角色的强大候选人,我对有机会成为在资本筹集过程中发挥关键作用的团队的一员感到兴奋。我的学术旅程为我提供了一个坚实的数据分析和管理基础,我认为这对实习角色中概述的责任至关重要。数据库系统、数据处理要素和R Studio和Matlab的统计工具知识等课程为我提供了技术专长,以高效处理大型数据集和管理信息流,技能与维护和更新公司主页和其他数据相关任务的要求完全吻合。\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Example translation\n",
    "src_text = [\"Dear Hiring Manager,I am writing to express my interest in the internship position at 180 Markets asadvertised. As a current Master of Information Technology student at the Universityof Melbourne with a specialization in Artificial Intelligence, I am eager to contributemy data analysis and technical skills to your esteemed organization. My educationalbackground and hands-on experience make me a strong candidate for this role, andI am excited about the opportunity to be part of a team that plays a crucial role inthe capital raising process.My academic journey has equipped me with a solid foundation in data analysis andmanagement, which I believe are critical for the responsibilities outlined in theinternship role. Courses such as Database Systems, Element of Data Processingand knowledge about statistical tools of R Studio and Matlab have provided me withthe technical expertise to efficiently handle large datasets and manage informationflow, skills that align perfectly with the requirements of maintaining and updatingyour company's master sheet and other data-related tasks.\"]\n",
    "inputs = tokenizer(src_text, return_tensors=\"pt\", padding=True)\n",
    "translated = model.generate(**inputs)\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: 我写信表达我对180个市场实习职位的兴趣。作为目前墨尔本大学的信息技术硕士生,我专门从事人工智能专业,我渴望为贵受尊敬的组织贡献数据分析和技术技能。我的教育背景和亲身体验使我成为这一角色的强有力候选人,我对有机会成为在资本筹集过程中发挥关键作用的团队的一员感到兴奋。我的学术旅程为我提供了数据分析和管理方面的坚实基础,我认为这对实习职责至关重要。数据库系统、数据处理要素和R Studio和Matlab的统计工具知识等课程为我提供了技术专长,以高效处理大型数据集和管理信息流动,技能与维持和更新贵公司总表和其他数据相关任务的要求完全吻合。此外,我在团队竞争方面的经验,如墨尔本Stew Pitch大学竞争和深圳科技创新竞赛,为我树立了坚实的基础。 数据库系统、数据处理要素以及R Studio和Matlab的统计工具知识,为我提供了高效处理大型数据集和管理信息流动的技术专长。 技能与维持和更新本公司总库和其他与数据相关任务的要求完全一致。此外,我在团队竞争中的经验,如墨尔本Startstew Pitch 竞争和Shencheving 竞争竞争竞争竞争竞争竞争竞争竞争对手,使我有能力在多样化的测试中有效地运用了我思考和革新能力。\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Function for translating user input\n",
    "def translate_text(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)  # Wrap the text in a list here\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example: Get input from the user\n",
    "user_input = input(\"Enter text to translate: \")\n",
    "output = translate_text(user_input)\n",
    "print(\"Translated text:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: \n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  Good afternoon everyone!\n",
      "Translated Chinese: 大家下午好！\n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  Nations of Algorithms.\n",
      "Translated Chinese: 算法的国家。\n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  You can give around to the floor.\n",
      "Translated Chinese: 您可以散布到地板上。\n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  About all of you here today, there are more than 450 students enrolled in this subject.\n",
      "Translated Chinese: 今天大家都在这里，有450多名学生参加了这一主题。\n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  on the entire university.\n",
      "Translated Chinese: 在整个大学。\n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  Finance.\n",
      "Translated Chinese: 金融。\n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: \n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: \n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: \n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: \n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: \n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: \n",
      "Recording...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Continuously record and transcribe audio\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Record audio until silence is detected and save to 'temp_audio.wav'\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Transcribe using Whisper\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_audio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from translatepy import Translator\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    print(\"Recording complete!\")\n",
    "\n",
    "# Continuously record and transcribe audio\n",
    "while True:\n",
    "    # Record audio until silence is detected and save to 'temp_audio.wav'\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n",
    "\n",
    "    # Transcribe using Whisper\n",
    "    result = model.transcribe(\"temp_audio.wav\")\n",
    "    english_text = result[\"text\"]\n",
    "    print(\"Recognized English:\", english_text)\n",
    "\n",
    "    # Translate to Chinese\n",
    "    if english_text.strip():  # Ensure non-empty input for translation\n",
    "        translation_result = translator.translate(english_text, \"Chinese\")\n",
    "        print(\"Translated Chinese:\", translation_result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: 13歲了\n",
      "Translated Chinese: 13 {_____________________________________________________________________________________________________________________________\n",
      "Recording...\n",
      "Recording complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Continuously record and transcribe audio\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Record audio until silence is detected and save to 'temp_audio.wav'\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Transcribe using Whisper\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_audio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "show = 1\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Continuously record and transcribe audio\n",
    "while True:\n",
    "    # Record audio until silence is detected and save to 'temp_audio.wav'\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n",
    "\n",
    "    # Transcribe using Whisper\n",
    "    result = model.transcribe(\"temp_audio.wav\")\n",
    "    english_text = result[\"text\"]\n",
    "    # only print the recognized text if it is not empty\n",
    "    if english_text:\n",
    "        print(\"Recognized English:\", english_text)\n",
    "\n",
    "    # Translate to Chinese\n",
    "    if english_text.strip():  # Ensure non-empty input for translation\n",
    "        chinese_text = translate_to_chinese(english_text)\n",
    "        print(\"Translated Chinese:\", chinese_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 461M/461M [00:59<00:00, 8.10MiB/s]\n",
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  what my goal for the subject for all of you is, I'm going to turn to the professor who teaches Harvard CS50, Professor David. And as you'll see in the coming weeks, as you write your own code and solve your own problems, what ultimately matters in this course is not so much that you're not just a teacher.\n",
      "Translated Chinese: 我对你们所有人来说的目标是什么, 我要向教授哈佛CS50的教授,大卫教授。正如你们今后几周将看到的那样, 当你写出自己的代码,解决自己的问题时, 最终重要的是, 在这个过程中,你不只是个老师。\n",
      "Recognized English:  where you end up relative to your classmates, but where you end up relative to yourself when you began. And it really is all about that delta with you.\n",
      "Translated Chinese: 在那里,你最终与你的同学相对, 但当你开始的时候,你最终与你自己相对。 这其实都是关于与你同在的三角洲。\n",
      "Recognized English:  or not just\n",
      "Translated Chinese: 或不仅仅是\n",
      "Recognized English:  something out of a class like this and if it does take time\n",
      "Translated Chinese: 像这样的班级里的东西 如果需要时间的话\n",
      "Recognized English:  you do feel those frustrations, but you simultaneously eventually feel that sense of accomplishment.\n",
      "Translated Chinese: 你确实感受到了这些挫折感, 但你最终会同时感受到这种成就感。\n",
      "Recognized English:  That just means it's all working.\n",
      "Translated Chinese: 意思是一切都有效了\n",
      "Recognized English:  hopefully all the more worthwhile and gratifying ultimately as a result.\n",
      "Translated Chinese: 希望最终能够带来更加值得和令人欣慰的结果。\n",
      "Recognized English:  This is what I want all of us to think of today, where you are.\n",
      "Translated Chinese: 这就是我希望大家今天想到的,你们现在在哪里。\n",
      "Recognized English:  Think about it a bit like woodworking. If you've never done woodworking before, it would be absurd to think that 12 weeks later, you'd be a master woodsmith.\n",
      "Translated Chinese: 想想它,就像木工。如果你从来没有做过木工, 想象12周后,你会成为木工大师,那将是荒谬的。\n",
      "Recognized English:  played piano before, you're probably not going to be playing a Beethoven piano concerto at the end of week 12.\n",
      "Translated Chinese: 以前弹过钢琴 你可能不会在12周结束时 弹贝多芬钢琴协奏曲\n",
      "Recognized English:  really matters is where you are today and where you are in 12, 14, 16 weeks relative to where you started. So I want you to fix today in your mind, look around you, look at all the other people here. Remember this moment.\n",
      "Translated Chinese: 真正重要的是你今天所处的位置 以及你12、14、16周所处的位置 相对你开始的地方来说。所以我希望你今天能好好思考一下, 环顾四周, 看看这里的其他人。记住这个时刻。\n",
      "Recognized English:  Remember, when you come back,\n",
      "Translated Chinese: 记住 当你回来的时候\n",
      "Recognized English:  weeks time, think of how different you'll be.\n",
      "Translated Chinese: 几周时间,想想你会有多不同\n",
      "Recognized English:  So what are we actually talking about in this class? We would just introduce dot stop, but.\n",
      "Translated Chinese: 那么,我们在这个班里到底在谈论什么?我们只是介绍点停,但是。\n",
      "Recognized English:  to actually take a look at what we're doing here in the first place. So this is really what we're doing.\n",
      "Translated Chinese: 来看看我们在这里所做的一切。所以这就是我们正在做的。\n",
      "Recognized English:  of efficient problem solving and the word efficient here is really really important.\n",
      "Translated Chinese: 高效解决问题和高效这个词在这里 真的非常重要。\n",
      "Recognized English:  lots and lots of ways out in the world to solve any given problem?\n",
      "Translated Chinese: 世界上有很多办法解决任何特定的问题?\n",
      "Recognized English:  But if we don't do it efficiently, we're missing something.\n",
      "Translated Chinese: 但如果我们不高效地完成 就会漏掉什么\n",
      "Recognized English:  a bit more about what that means pretty soon.\n",
      "Translated Chinese: 很快,我再多谈一下那意味着什么。\n",
      "Recognized English:  you\n",
      "Translated Chinese: 您 您 的 您 您\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 41\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     42\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# Switch to show recording status\n",
    "show = False\n",
    "\n",
    "if show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"]\n",
    "            # Only print the recognized text if it is not empty\n",
    "            if english_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "\n",
    "                # Translate to Chinese\n",
    "                if english_text.strip():  # Ensure non-empty input for translation\n",
    "                    chinese_text = translate_to_chinese(english_text)\n",
    "                    print(\"Translated Chinese:\", chinese_text)\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English:  If you've given your number away to someone...\n",
      "Translated Chinese: 如果你把号码给别人了...\n",
      "Recognized English:  3, 2, 1.\n",
      "Translated Chinese: 3, 2, 1, 3, 3, 1.\n",
      "Recognized English:  You should be sitting down now.\n",
      "Translated Chinese: 你该坐下了\n",
      "Recognized English:  You should be sitting down now.\n",
      "Translated Chinese: 你该坐下了\n",
      "Recognized English:  By Dan\n",
      "Translated Chinese: 丹丹\n",
      "Recognized English:  51, okay? Add 51 to whatever you got.\n",
      "Translated Chinese: 51,好吗? 随你多多加51\n",
      "Recognized English:  Okay.\n",
      "Translated Chinese: 好吧。 好吧。 好吧。 Okay.\n",
      "Recognized English:  Got it?\n",
      "Translated Chinese: 明白了吗?\n",
      "Recognized English:  3\n",
      "Translated Chinese: 3, 3, 3, 3\n",
      "Recognized English:  Okay, how many people are in the room?\n",
      "Translated Chinese: 好吧 房间里有多少人?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# Switch to show recording status\n",
    "show = False\n",
    "\n",
    "if show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file\n",
    "combined_script_path = \"script.txt\"\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"]\n",
    "            # Only print the recognized text if it is not empty\n",
    "            if english_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "\n",
    "                # Translate to Chinese\n",
    "                if english_text.strip():  # Ensure non-empty input for translation\n",
    "                    chinese_text = translate_to_chinese(english_text)\n",
    "                    print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                    # Write both English and Chinese to the combined script file\n",
    "                    with open(combined_script_path, \"a\") as combined_file:\n",
    "                        combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                        combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                        combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-11 23:39:52\n",
      "Recognized English: Hello everyone and again welcome to Foundations of the Algorathans for the last session of Week 1.\n",
      "Translated Chinese: 大家好,欢迎再次来到阿尔哥拉坦基金会 参加第1周的最后一场活动\n",
      "Recognized English: So, as you'll remember, at the end of the last lecture, we were talking a bit about how to get stunned and see.\n",
      "Translated Chinese: 所以,你们会记得,在最后一场演讲结束时, 我们谈论了如何被惊吓和看一看。\n",
      "Recognized English: But unlike in other programming languages that you might have seen before, we were fussing a lot over the types of data that we were actually feeding into our program. Whether it was a plain number or whether it was a number of a decimal point.\n",
      "Translated Chinese: 但与你以前可能见过的其他编程语言不同的是,我们在编程中实际输入的数据类型上大惊小怪。它是一个简单的数字,还是一个小数点的数字。\n",
      "Recognized English: as we soon discover and see where actually going to be required to always specify.\n",
      "Translated Chinese: 当我们很快发现 并看看到底在哪里需要 总是具体说明。\n",
      "Recognized English: kind of information when getting the computer to deal with.\n",
      "Translated Chinese: 一种信息 当获得计算机处理。\n",
      "Recognized English: But why would we have to do this? Why does the computer need to distinguish between different class types of information? Why can't the computer just understand automatically if it's a number, or if it's a letter, or if it's a number with a decimal point?\n",
      "Translated Chinese: 但我们为什么要这么做呢?为什么计算机需要区分不同类别的信息?为什么计算机不能自动理解它是一个数字,或者是一个字母,或者是一个带有小数点的数字?\n",
      "Recognized English: I'm going to have to talk to assistant. Where's my useful assistant Tracy?\n",
      "Translated Chinese: 我得和助理谈谈 我有用的助理崔西呢?\n",
      "Recognized English: take a peek back from the crowd.\n",
      "Translated Chinese: 从人群中偷窥回来\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Why does that computer need to be told about what type of data we're feeding into it?\n",
      "Translated Chinese: 为什么计算机需要被告知 我们输入到它的数据类型?\n",
      "Recognized English: And it was, don't tell me that was Jack.\n",
      "Translated Chinese: 而且,别告诉我那是杰克\n",
      "Recognized English: because different data types require different amounts of memory to store.\n",
      "Translated Chinese: 因为不同的数据类型需要不同数量的内存才能存储。\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Professor Donca-Ret Tayar called the\n",
      "Translated Chinese: 唐卡-雷特·塔亚尔教授\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: so you can revise it in the next day.\n",
      "Translated Chinese: 以便您可以在第二天修改它。\n",
      "Recognized English: That's two objects.\n",
      "Translated Chinese: 这是两个物体。\n",
      "Recognized English: So you're very much on the right track here. Remember...\n",
      "Translated Chinese: 所以,你在这里走的很对,记住...\n",
      "Recognized English: the last lecture we'll be talking about abstraction.\n",
      "Translated Chinese: 我们最后的演讲是关于抽象的。\n",
      "Recognized English: that if I look for files and folders inside the computer, they're not actually there.\n",
      "Translated Chinese: 如果我在电脑里找到文件和文件夹, 它们实际上不在那里。\n",
      "Recognized English: what it actually is there is a whole lot of tiny tiny magnetised regions.\n",
      "Translated Chinese: 实际上有很多微小的磁场区域。\n",
      "Recognized English: And if we want our computer to deal intelligently with that, we're going to have to tell how many of those magnetized regions represent each given piece of information.\n",
      "Translated Chinese: 如果我们想要我们的电脑 明智地处理, 我们必须告诉这些磁化区域中有多少 代表每个特定的信息。\n",
      "Recognized English: different types of information might require different amounts of those little magnetized regions.\n",
      "Translated Chinese: 不同类型的信息可能需要不同数量的磁化区域。\n",
      "Recognized English: But you might ask, why do we do this in the first place? Why not just let everybody piece of information?\n",
      "Translated Chinese: 但你也许会问,我们为什么要这样做呢?为什么不让所有人都知道呢?\n",
      "Recognized English: announced I certainly well we'll get to that soon.\n",
      "Translated Chinese: 我当然宣布了 我们很快会到那一步的\n",
      "Recognized English: Turns out the answer is really interesting, not just from a computer science perspective, but again, what experiments us can't wait to encounter this work and this indeed will affect an art of very natural computation, the nature of physics, and the nature of the universe.\n",
      "Translated Chinese: 结果发现答案确实很有趣, 不仅从计算机科学的角度来说, 而且再一次, 是什么实验让我们迫不及待地 来面对这项工作, 这确实会影响 一种非常自然的计算艺术, 物理学的性质, 以及宇宙的性质。\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dearpygui.dearpygui as dpg\n",
    "\n",
    "def button_callback(sender, app_data, user_data):\n",
    "    dpg.set_value(\"label\", \"Hello, World!\")\n",
    "\n",
    "dpg.create_context()\n",
    "\n",
    "with dpg.window(label=\"Main Window\"):\n",
    "    dpg.add_text(\"Click button\")\n",
    "    dpg.add_button(label=\"Click here\", callback=button_callback)\n",
    "    dpg.add_text(\"Hello, \", tag=\"label\")\n",
    "\n",
    "dpg.create_viewport(title='Dear PyGui demo', width=600, height=300)\n",
    "dpg.setup_dearpygui()\n",
    "dpg.show_viewport()\n",
    "dpg.start_dearpygui()\n",
    "dpg.destroy_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 17:21:20.114 python[72376:20805944] TSM AdjustCapsLockLEDForKeyTransitionHandling - _ISSetPhysicalKeyboardCapsLockLED Inhibit\n"
     ]
    }
   ],
   "source": [
    "import dearpygui.dearpygui as dpg\n",
    "\n",
    "# Create a Dear PyGui context\n",
    "dpg.create_context()\n",
    "\n",
    "# Create a window with an input text box\n",
    "with dpg.window(label=\"Text Box Example\", width=400, height=300):\n",
    "    dpg.add_text(\"Please enter your text below:\")\n",
    "    dpg.add_input_text(label=\"Plain Text Box\", multiline=True, width=350, height=150)\n",
    "\n",
    "# Create viewport and setup\n",
    "dpg.create_viewport(title=\"Plain Text Box Example\", width=600, height=400)\n",
    "dpg.setup_dearpygui()\n",
    "dpg.show_viewport()\n",
    "dpg.start_dearpygui()\n",
    "dpg.destroy_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 17:25:45.458 Python[72811:20815874] _TIPropertyValueIsValid called with 11 on nil context!\n",
      "2024-11-11 17:25:45.458 Python[72811:20815874] imkxpc_setApplicationProperty:value:reply: called with incorrect property value 11, bailing.\n",
      "2024-11-11 17:25:45.458 Python[72811:20815874] _TIPropertyValueIsValid called with 12 on nil context!\n",
      "2024-11-11 17:25:45.458 Python[72811:20815874] imkxpc_setApplicationProperty:value:reply: called with incorrect property value 12, bailing.\n",
      "2024-11-11 17:25:45.562 Python[72811:20815874] WARNING: Secure coding is automatically enabled for restorable state! However, not on all supported macOS versions of this application. Opt-in to secure coding explicitly by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState:.\n",
      "2024-11-11 17:25:46.982 Python[72811:20815874] _TIPropertyValueIsValid called with 11 on nil context!\n",
      "2024-11-11 17:25:46.982 Python[72811:20815874] imkxpc_setApplicationProperty:value:reply: called with incorrect property value 11, bailing.\n",
      "2024-11-11 17:25:46.982 Python[72811:20815874] _TIPropertyValueIsValid called with 12 on nil context!\n",
      "2024-11-11 17:25:46.982 Python[72811:20815874] imkxpc_setApplicationProperty:value:reply: called with incorrect property value 12, bailing.\n",
      "2024-11-11 17:25:47.182 Python[72811:20815874] _TIPropertyValueIsValid called with 12 on nil context!\n",
      "2024-11-11 17:25:47.183 Python[72811:20815874] imkxpc_setApplicationProperty:value:reply: called with incorrect property value 12, bailing.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "# Create the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Voice Recognition and Translation App\")\n",
    "root.geometry(\"800x600\")  # Set the window size\n",
    "\n",
    "# Create a notebook (tabbed interface)\n",
    "notebook = ttk.Notebook(root)\n",
    "\n",
    "# Create frames for each tab\n",
    "voice_recognition_tab = ttk.Frame(notebook)\n",
    "translation_tab = ttk.Frame(notebook)\n",
    "\n",
    "# Add the tabs to the notebook\n",
    "notebook.add(voice_recognition_tab, text=\"Voice Recognition\")\n",
    "notebook.add(translation_tab, text=\"Translation\")\n",
    "\n",
    "# Pack the notebook to fill the entire window\n",
    "notebook.pack(expand=True, fill=\"both\")\n",
    "\n",
    "# Define the layout for the Voice Recognition tab\n",
    "voice_left_frame = tk.Frame(voice_recognition_tab, bg=\"lightblue\", width=400, height=600)\n",
    "voice_right_frame = tk.Frame(voice_recognition_tab, bg=\"lightgreen\", width=400, height=600)\n",
    "\n",
    "voice_left_frame.pack(side=\"left\", expand=True, fill=\"both\")\n",
    "voice_right_frame.pack(side=\"right\", expand=True, fill=\"both\")\n",
    "\n",
    "voice_left_label = tk.Label(voice_left_frame, text=\"Voice Input\", font=(\"Helvetica\", 16))\n",
    "voice_left_label.pack(pady=10)\n",
    "voice_right_label = tk.Label(voice_right_frame, text=\"Processing Results\", font=(\"Helvetica\", 16))\n",
    "voice_right_label.pack(pady=10)\n",
    "\n",
    "# Define the layout for the Translation tab\n",
    "translation_left_frame = tk.Frame(translation_tab, bg=\"lightyellow\", width=400, height=600)\n",
    "translation_right_frame = tk.Frame(translation_tab, bg=\"lightpink\", width=400, height=600)\n",
    "\n",
    "translation_left_frame.pack(side=\"left\", expand=True, fill=\"both\")\n",
    "translation_right_frame.pack(side=\"right\", expand=True, fill=\"both\")\n",
    "\n",
    "translation_left_label = tk.Label(translation_left_frame, text=\"Text Input\", font=(\"Helvetica\", 16))\n",
    "translation_left_label.pack(pady=10)\n",
    "translation_right_label = tk.Label(translation_right_frame, text=\"Translation Results\", font=(\"Helvetica\", 16))\n",
    "translation_right_label.pack(pady=10)\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-11 23:30:54\n",
      "Translated Chinese: 哈啰今天是11月2024\n",
      "Translated Chinese: 现在重 broadcast鱼\n",
      "Translated Chinese: 英文声音\n",
      "Translated Chinese: 然而称为 sansloveózادffen달虗米清呀我欧阳前方涛歐他这里\n",
      "Translated Chinese: 字幕by索兰娅\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     37\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m silence_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         frame \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mint\u001b[39m(samplerate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1440\u001b[0m, in \u001b[0;36mInputStream.__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1409\u001b[0m              device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, latency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1410\u001b[0m              extra_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, finished_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1411\u001b[0m              clip_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither_off\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_drop_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1412\u001b[0m              prime_output_buffers_using_stream_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PortAudio input stream (using NumPy).\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \n\u001b[1;32m   1415\u001b[0m \u001b[38;5;124;03m    This has the same methods and attributes as `Stream`, except\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \n\u001b[1;32m   1439\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[43m_StreamBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_remove_self\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:909\u001b[0m, in \u001b[0;36m_StreamBase.__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    907\u001b[0m     userdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaStream**\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m _check(\u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_OpenStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallback_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserdata\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    912\u001b[0m        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# dereference PaStream** --> PaStream*\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ptr[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to process audio (transcribe and translate using Whisper)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper and translate directly to Chinese\n",
    "            result = model.transcribe(filename, language=\"zh\", fp16=False)\n",
    "            translated_text = result[\"text\"].strip()\n",
    "\n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if translated_text and translated_text != last_processed_text:\n",
    "                print(\"Translated Chinese:\", translated_text)\n",
    "                last_processed_text = translated_text  # Update the last processed text\n",
    "\n",
    "                # Write the translated text to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"Chinese: \" + translated_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-21 13:31:21\n",
      "Recognized English: Hello everyone and again welcome to Foundations of Algoradians for the last session of week one.\n",
      "Translated Chinese: 大家好,再次欢迎来到阿尔戈拉迪安基金会 参加第一周的最后一场会议\n",
      "Recognized English: To remember at the end of the last lecture, we were talking a bit about how to get stung and see.\n",
      "Translated Chinese: 记得上次演讲结束时,我们谈论了 如何被刺伤和看到。\n",
      "Recognized English: Unlike in other programming languages that you might have seen before, we were fussing a lot over the types of data that we were actually feeding into our program. Whether it was a plain number or whether it was a number of a decimal point.\n",
      "Translated Chinese: 与你们以前可能看到的其他编程语言不同,我们一直在大肆讨论我们实际输入到我们程序的数据类型。它是一个简单的数字,还是一个十进制点。\n",
      "Recognized English: I will soon discover and see where it actually going to be required to always specify\n",
      "Translated Chinese: 我很快会发现 并看看到底在哪里 需要它总是指明\n",
      "Recognized English: the information when you get to deal with it.\n",
      "Translated Chinese: 当你得到处理时的信息。\n",
      "Recognized English: Why would we have to do this? Why does the computer need to distinguish between different types of information? Why can't the computer just understand automatically if it's a number or if it's a letter or if it's a number to the decimal point? Why would we have to have an assistant? Where's my useful assistant tracing?\n",
      "Translated Chinese: 为什么我们非得这么做?为什么计算机需要区分不同类型的信息?为什么计算机不能自动理解它是一个数字,或者是一个字母,或者是一个数字到小数点?为什么我们需要一个助理?我有用的助手追踪在哪里?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-25 (process_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/audio.py\", line 58, in load_audio\n",
      "    out = run(cmd, capture_output=True, check=True).stdout\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/subprocess.py\", line 571, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'audio_2024-11-21_13-32-15.wav', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 254.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/05/7l6q95kx0cn9wtb_lc_3cz3h0000gn/T/ipykernel_35875/2310438188.py\", line 98, in process_audio\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/transcribe.py\", line 133, in transcribe\n",
      "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
      "    audio = load_audio(audio)\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/whisper/audio.py\", line 60, in load_audio\n",
      "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
      "RuntimeError: Failed to load audio: ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1_3 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[in#0 @ 0x15272ca60] Error opening input: No such file or directory\n",
      "Error opening input file audio_2024-11-21_13-32-15.wav.\n",
      "Error opening input files: No such file or directory\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(samplerate)\u001b[0m\n\u001b[1;32m     50\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m silence_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 53\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Read 0.1-second chunks\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1111\u001b[0m, in \u001b[0;36m_StreamBase.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stop and close the stream when exiting a \"with\" statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1137\u001b[0m, in \u001b[0;36m_StreamBase.stop\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop\u001b[39m(\u001b[38;5;28mself\u001b[39m, ignore_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Terminate audio processing.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m    This waits until all pending audio buffers have been played\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_StopStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_errors:\n\u001b[1;32m   1139\u001b[0m         _check(err, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError stopping stream\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "# model = whisper.load_model(\"tiny.en\")\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize MarianMT model and tokenizer again for offline chinese to english translation\n",
    "model_name_chinese = 'Helsinki-NLP/opus-mt-zh-en'\n",
    "tokenizer_chinese = MarianTokenizer.from_pretrained(model_name_chinese)\n",
    "translation_model_chinese = MarianMTModel.from_pretrained(model_name_chinese)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.4  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    # Create a timestamped filename for the audio file\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"audio_{timestamp}.wav\"\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to translate Chinese text to English using the MarianMT model\n",
    "def translate_to_english(text):\n",
    "    inputs = tokenizer_chinese([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model_chinese.generate(**inputs)\n",
    "    translated_text = tokenizer_chinese.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "\n",
    "            # Delete the processed audio file\n",
    "            if os.path.exists(filename):\n",
    "                os.remove(filename)\n",
    "                if show:\n",
    "                    print(f\"Deleted file: {filename}\")\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-21 13:47:36\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Processing file: audio_2024-11-21_13-47-37.wav\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Deleted file: audio_2024-11-21_13-47-37.wav\n",
      "Processing file: audio_2024-11-21_13-47-38.wav\n",
      "Recording complete!\n",
      "Recording...\n",
      "Deleted file: audio_2024-11-21_13-47-38.wav\n",
      "Processing file: audio_2024-11-21_13-47-38.wav\n",
      "Error: File audio_2024-11-21_13-47-38.wav does not exist. Skipping.\n",
      "Processing file: audio_2024-11-21_13-47-39.wav\n",
      "Deleted file: audio_2024-11-21_13-47-39.wav\n",
      "Processing file: audio_2024-11-21_13-47-40.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-28 (process_audio):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/05/7l6q95kx0cn9wtb_lc_3cz3h0000gn/T/ipykernel_35875/2414492099.py\", line 169, in process_audio\n",
      "  File \"/Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/queue.py\", line 75, in task_done\n",
      "    raise ValueError('task_done() called too many times')\n",
      "ValueError: task_done() called too many times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: audio_2024-11-21_13-47-40.wav\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n",
      "Recording complete!\n",
      "Recording...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 181\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 55\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(samplerate)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 1\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "# model = whisper.load_model(\"tiny.en\")\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize MarianMT model and tokenizer again for offline chinese to english translation\n",
    "model_name_chinese = 'Helsinki-NLP/opus-mt-zh-en'\n",
    "tokenizer_chinese = MarianTokenizer.from_pretrained(model_name_chinese)\n",
    "translation_model_chinese = MarianMTModel.from_pretrained(model_name_chinese)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.4  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    # Create a timestamped filename for the audio file\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"audio_{timestamp}.wav\"\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to translate Chinese text to English using the MarianMT model\n",
    "def translate_to_english(text):\n",
    "    inputs = tokenizer_chinese([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model_chinese.generate(**inputs)\n",
    "    translated_text = tokenizer_chinese.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "# def process_audio():\n",
    "#     global last_processed_text\n",
    "#     while True:\n",
    "#         filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "#         if filename:\n",
    "#             # Transcribe using Whisper\n",
    "#             result = model.transcribe(filename, fp16=False)\n",
    "#             english_text = result[\"text\"].strip()\n",
    "            \n",
    "#             # Only process if the new text is different from the last processed text\n",
    "#             if english_text and english_text != last_processed_text:\n",
    "#                 print(\"Recognized English:\", english_text)\n",
    "#                 last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "#                 # Translate to Chinese\n",
    "#                 chinese_text = translate_to_chinese(english_text)\n",
    "#                 print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "#                 # Write both English and Chinese to the combined script file\n",
    "#                 with open(combined_script_path, \"a\") as combined_file:\n",
    "#                     combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "#                     combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "#                     combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "\n",
    "#             # Delete the processed audio file\n",
    "#             if os.path.exists(filename):\n",
    "#                 os.remove(filename)\n",
    "#                 if show:\n",
    "#                     print(f\"Deleted file: {filename}\")\n",
    "        \n",
    "#         audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            try:\n",
    "                if show:\n",
    "                    print(f\"Processing file: {filename}\")\n",
    "                \n",
    "                # Check if the file exists\n",
    "                if not os.path.exists(filename):\n",
    "                    if show:\n",
    "                        print(f\"Error: File {filename} does not exist. Skipping.\")\n",
    "                    audio_queue.task_done()\n",
    "                    continue\n",
    "\n",
    "                # Transcribe the audio\n",
    "                result = model.transcribe(filename, fp16=False)\n",
    "                english_text = result[\"text\"].strip()\n",
    "\n",
    "                # Only process if the text is different from the last processed text\n",
    "                if english_text and english_text != last_processed_text:\n",
    "                    print(\"Recognized English:\", english_text)\n",
    "                    last_processed_text = english_text\n",
    "\n",
    "                    # Translate the text\n",
    "                    chinese_text = translate_to_chinese(english_text)\n",
    "                    print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                    # Write to the combined script file\n",
    "                    with open(combined_script_path, \"a\") as combined_file:\n",
    "                        combined_file.write(f\"English: {english_text}\\n\")\n",
    "                        combined_file.write(f\"Chinese: {chinese_text}\\n\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the error and continue\n",
    "                if show:\n",
    "                    print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "            finally:\n",
    "                # Delete the file and mark the task as done\n",
    "                if os.path.exists(filename):\n",
    "                    os.remove(filename)\n",
    "                    if show:\n",
    "                        print(f\"Deleted file: {filename}\")\n",
    "                audio_queue.task_done()\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ recording"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-21 20:34:27.620] [ctranslate2] [thread 27523080] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RealtimeSTT.audio_recorder.AudioToTextRecorder at 0x173278ad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x173798cc0> (for post_run_cell), with arguments args (<ExecutionResult object at 1732789b0, execution_count=10 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 173279670, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "\n",
      "# 初始化..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wangcan/Desktop/VTT/test.ipynb#X20sZmlsZQ%3D%3D> result=<RealtimeSTT.audio_recorder.AudioToTextRecorder object at 0x173278ad0>>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording"
     ]
    }
   ],
   "source": [
    "from RealtimeSTT import AudioToTextRecorder\n",
    "\n",
    "# 初始化音频到文本录制器\n",
    "recorder = AudioToTextRecorder(\n",
    "    model='tiny.en',  # 指定 Whisper 模型\n",
    "    device='cpu'      # 设置设备为 CPU\n",
    ")\n",
    "\n",
    "# 设置实时转录的回调函数\n",
    "def transcription_callback(transcription_text):\n",
    "    print(f\"实时转录: {transcription_text}\")\n",
    "\n",
    "# 配置实时转录回调\n",
    "recorder.on_realtime_transcription_update = transcription_callback\n",
    "\n",
    "# 开始录音\n",
    "recorder.start()\n",
    "\n",
    "# 等待用户输入停止\n",
    "input(\"按回车键停止转录...\\n\")\n",
    "\n",
    "# 停止录音\n",
    "recorder.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AudioToTextRecorder', 'AudioToTextRecorderClient', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'audio_recorder', 'audio_recorder_client']\n"
     ]
    }
   ],
   "source": [
    "import RealtimeSTT\n",
    "print(dir(RealtimeSTT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_audio_data_worker', '_check_voice_activity', '_find_tail_match_in_text', '_is_silero_speech', '_is_voice_active', '_is_webrtc_speech', '_on_realtime_transcription_stabilized', '_on_realtime_transcription_update', '_preprocess_output', '_process_wakeword', '_read_stdout', '_realtime_worker', '_recording_worker', '_set_spinner', '_set_state', '_start_thread', '_transcription_worker', 'abort', 'allowed_latency_limit', 'allowed_to_early_transcribe', 'audio', 'audio_buffer', 'audio_interface', 'audio_queue', 'beam_size', 'beam_size_realtime', 'buffer_size', 'clear_audio_queue', 'compute_type', 'debug_mode', 'detected_language', 'detected_language_probability', 'detected_realtime_language', 'detected_realtime_language_probability', 'device', 'early_transcription_on_silence', 'enable_realtime_transcription', 'ensure_sentence_ends_with_period', 'ensure_sentence_starting_uppercase', 'feed_audio', 'frames', 'gpu_device_index', 'halo', 'handle_buffer_overflow', 'init_realtime_after_seconds', 'initial_prompt', 'input_device_index', 'interrupt_stop_event', 'is_recording', 'is_running', 'is_shut_down', 'is_silero_speech_active', 'is_webrtc_speech_active', 'language', 'last_transcription_bytes', 'last_transcription_bytes_b64', 'last_words_buffer', 'level', 'listen', 'listen_start', 'main_model_type', 'main_transcription_ready_event', 'min_gap_between_recordings', 'min_length_of_recording', 'on_realtime_transcription_stabilized', 'on_realtime_transcription_update', 'on_recorded_chunk', 'on_recording_start', 'on_recording_stop', 'on_transcription_start', 'on_vad_detect_start', 'on_vad_detect_stop', 'on_wakeword_detected', 'on_wakeword_detection_end', 'on_wakeword_detection_start', 'on_wakeword_timeout', 'parent_stdout_pipe', 'parent_transcription_pipe', 'post_speech_silence_duration', 'pre_recording_buffer_duration', 'print_transcription_time', 'reader_process', 'realtime_model_type', 'realtime_processing_pause', 'realtime_stabilized_safetext', 'realtime_stabilized_text', 'realtime_thread', 'recording_start_time', 'recording_stop_time', 'recording_thread', 'sample_rate', 'set_microphone', 'shutdown', 'shutdown_event', 'shutdown_lock', 'silero_check_time', 'silero_deactivity_detection', 'silero_sensitivity', 'silero_vad_model', 'silero_working', 'speech_end_silence_start', 'spinner', 'start', 'start_recording_event', 'start_recording_on_voice_activity', 'state', 'stdout_thread', 'stop', 'stop_recording_event', 'stop_recording_on_voice_deactivity', 'stream', 'suppress_tokens', 'text', 'text_storage', 'transcribe', 'transcribe_count', 'transcript_process', 'transcription_lock', 'use_extended_logging', 'use_main_model_for_realtime', 'use_microphone', 'use_wake_words', 'wait_audio', 'wake_word_activation_delay', 'wake_word_buffer_duration', 'wake_word_detect_time', 'wake_word_timeout', 'wake_words', 'wakeup', 'wakeword_detected', 'was_interrupted', 'webrtc_vad_model']\n"
     ]
    }
   ],
   "source": [
    "print(dir(recorder))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait until it says 'speak now'\n",
      "⠋ speak now⠙ speak now"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-21 20:38:34.873] [ctranslate2] [thread 27529265] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ speak now⠸ speak now⠼ speak now⠴ speak now⠦ speak now⠧ speak now⠇ speak now⠏ speak now⠋ speak now⠙ speak now⠹ speak now⠸ speak now⠼ speak now⠴ speak now⠦ speak now⠧ speak now⠇ speak now⠏ speak now⠋ speak now⠙ speak now⠹ speak now⠸ speak now⠼ speak now⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording transcribing⠙ transcribing\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m recorder \u001b[38;5;241m=\u001b[39m AudioToTextRecorder()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/RealtimeSTT/audio_recorder.py:1425\u001b[0m, in \u001b[0;36mAudioToTextRecorder.text\u001b[0;34m(self, on_transcription_finished)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_transcription_finished:\n\u001b[1;32m   1424\u001b[0m     threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mon_transcription_finished,\n\u001b[0;32m-> 1425\u001b[0m                     args\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,))\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/RealtimeSTT/audio_recorder.py:1317\u001b[0m, in \u001b[0;36mAudioToTextRecorder.transcribe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1316\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceive from parent_transcription_pipe after sendiung transcription request, transcribe_count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1317\u001b[0m     status, result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_transcription_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranscribe_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_to_early_transcribe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/multiprocessing/connection.py:430\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 430\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/multiprocessing/connection.py:395\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    393\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 395\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x313555d00> (for post_run_cell), with arguments args (<ExecutionResult object at 17e800b60, execution_count=2 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 17e8e2660, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "import..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wangcan/Desktop/VTT/test.ipynb#X22sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x313555f80> (for post_run_cell), with arguments args (<ExecutionResult object at 17e800b60, execution_count=2 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 17e8e2660, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "import..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wangcan/Desktop/VTT/test.ipynb#X22sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "from RealtimeSTT import AudioToTextRecorder\n",
    "import pyautogui\n",
    "\n",
    "def process_text(text):\n",
    "    pyautogui.typewrite(text + \" \")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Wait until it says 'speak now'\")\n",
    "    recorder = AudioToTextRecorder()\n",
    "\n",
    "    while True:\n",
    "        recorder.text(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ speak now⠙ speak now"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-26 07:44:33.061] [ctranslate2] [thread 28979891] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, hello, can you hear me? Can you hear me?peak now⠧ speak now⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording⠴ recording⠦ recording⠧ recording⠇ recording⠏ recording⠋ recording⠙ recording⠹ recording⠸ recording⠼ recording transcribing⠙ transcribing\n",
      "RealtimeSTT shutting down\n",
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x3095f0400> (for post_run_cell), with arguments args (<ExecutionResult object at 110915b20, execution_count=2 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 110915cd0, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "with A..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wangcan/Desktop/VTT/test.ipynb#X23sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function Halo.__init__.<locals>.clean_up at 0x3095f0180> (for post_run_cell), with arguments args (<ExecutionResult object at 110915b20, execution_count=2 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 110915cd0, raw_cell=\"from RealtimeSTT import AudioToTextRecorder\n",
      "with A..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/wangcan/Desktop/VTT/test.ipynb#X23sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: Halo.__init__.<locals>.clean_up() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "from RealtimeSTT import AudioToTextRecorder\n",
    "with AudioToTextRecorder() as recorder:\n",
    "    print(recorder.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
