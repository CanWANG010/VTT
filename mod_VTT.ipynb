{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    if show:\n",
    "        print(\"Recording complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-13 23:54:45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 123\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: Hello hello hello\n",
      "Translated Chinese: 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好 你好\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    \n",
    "    # Reduce noise by applying a noise gate or filter\n",
    "    reduced_noise_audio = audio_segment - 10  # Reduces volume, acting as a simple noise gate\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    reduced_noise_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording and noise reduction complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-13 23:57:34\n",
      "Recognized English: Thanks for watching!\n",
      "Translated Chinese: 感谢观看!\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Get some of the highlights of what the Supreme Court has said. Look, in their verdict, they've made it very clear that the state is accountable for this kind of bulldozer justice. But have they also spoken about what needs to be done? What kind of a crackdown perhaps we could see or what the victims here of bulldozer justice will get?\n",
      "Translated Chinese: 获取最高法院所言的一些要点。在他们的判决中,他们已经明确表示国家要为这种推土机的正义负责。但他们是否也谈到需要做些什么?我们或许能看到什么样的镇压或者推土机正义的受害者会得到什么?\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: ये सुख्षिताया बारहें हतो � happily will see a kind of guidelines which will be applicable for Paula concentration in the country\n",
      "Translated Chinese: 很高兴能看到一种适用于 Paula 集中在这个国家的准则\n",
      "Recognized English: be happening across the country as far as the demolitions are concerned. The Supreme Court has made it very, very clear that a house is not a house and a house is actually\n",
      "Translated Chinese: 最高法院已经非常非常非常非常非常明确地指出,一栋房子不是一栋房子,一栋房子实际上是一栋房子\n",
      "Recognized English: I was a hope for the family members who stay there. You cannot snatch away the fundamental rights of any individual just because he is a convict or he is involved, allegedly involved in any kind of crime whatsoever it is. So Supreme Court has laid down certain guidelines. The guidelines are yet to be known to us because the verdict is still being read out in the open court and the verdict once come out in the public domain. We get to hear what exactly is the guideline which across the country means.\n",
      "Translated Chinese: 我曾经对住在那里的家属抱有希望。你不能仅仅因为他是罪犯或参与犯罪而剥夺任何个人的基本权利,据称他参与了任何类型的犯罪。因此,最高法院已经制定了某些指导方针。指导方针还有待我们了解,因为判决仍然在公开法庭宣读,裁决一旦公布在公共领域。我们开始了解全国的指南究竟意味着什么。\n",
      "Recognized English: implemented all the civic authorities need to need to abide by those laws but what we are gathering from whatever inputs my\n",
      "Translated Chinese: 执行所有民政当局都需要遵守这些法律,\n",
      "Recognized English: the Supreme Court will make it mandatory for all the authorities across the country to stop the violence.\n",
      "Translated Chinese: 最高法院将强制全国所有当局停止暴力。\n",
      "Recognized English: अग, नोटेस, तो तो बुग।\n",
      "Translated Chinese: , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "Recognized English: house owner, whosoever it is, whether he is involved in any crime or not, or whether he is a normal individual who has violated...\n",
      "Translated Chinese: 房主,不管是谁,不管他是否参与任何罪行,或者他是否是侵犯...的正常人。\n",
      "Recognized English: Law asks for the construction of the house is concerned.\n",
      "Translated Chinese: 有关建造房屋的法律要求。\n",
      "Recognized English: We need to inform the particular person whose house is being targeted in advance. The timeline would be decided by the top court in the particular guidelines. But the Supreme Court might not know. Just to clarify one issue here, while they've issued guidelines.\n",
      "Translated Chinese: 我们需要提前通知房屋被瞄准的某个人。 时间由最高法院在特定的指导方针中决定。 但最高法院可能不知道。 仅仅在这里澄清一个问题, 而他们已经发布了指导方针。\n",
      "Recognized English: Evidently, you see always the authorities saying that this property was illegal and that's why we resorted to demolishing it. What is the Supreme Court in its guidelines spoken about that particular point about whether property is...\n",
      "Translated Chinese: 显然,你总是看到当局说这些财产是非法的,这就是我们为何要拆除这些财产的原因。最高法院在其准则中谈到的关于财产是否是...\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Thanks for watching!\n",
      "Translated Chinese: 感谢观看!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    \n",
    "    # Reduce noise by lowering the volume\n",
    "    reduced_noise_audio = audio_segment - 10  # Reduces volume as a simple noise gate\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    normalized_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, and normalization complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:04:49\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Actually the authorities will have to clarify this through the, that what is the illegality involved, what is the illegal structure, why are they demolishing the entire property and not the illegal part, why they were involved.\n",
      "Translated Chinese: 实际上,当局必须通过以下方式澄清这一点,即所涉的非法性、非法结构、为何拆毁整个财产而不是非法部分、为何涉及非法部分。\n",
      "Recognized English: for all these years because the person gets convicted or gets involved in any kind of crime in that particular state the action comes\n",
      "Translated Chinese: 这些年来,由于某人被定罪或卷入特定国家的任何犯罪,诉讼就发生在该州。\n",
      "Recognized English: All these things need to be clarified to the authorities because now the guy is...\n",
      "Translated Chinese: 所有这些事情都需要向当局澄清 因为现在这家伙...\n",
      "Recognized English: will be in place and the authorities will have, you know, will be a question over as to why they have taken.\n",
      "Translated Chinese: 将就位 当局将会有,你知道, 将是一个问题 关于为什么他们 采取了。\n",
      "Recognized English: action against that particular property. That particular property could be illegal earlier also but since the person is now involved in any kind of crime, this kind of bulldozer justice that we usually call that should be stopped. This is the case that actually come before the top court only because some people from Rajasthan and Uttar Pradesh have approached the top court saying that since these people were involved in...\n",
      "Translated Chinese: 这种特定财产可能更早地是非法的,但既然此人现在参与任何类型的犯罪,我们通常称之为这种推土机正义的这种推土机正义就应该停止。 这实际上只是因为来自拉贾斯坦邦和北方邦的一些人向最高法院提出,说既然这些人参与...\n",
      "Recognized English: The state government got the Bulldozer justice against them and that is why the Supreme Court was forced to bring out such guidelines to meet their authority.\n",
      "Translated Chinese: 州政府得到了推土机正义 对他们, 这就是为什么最高法院 被迫提出这样的准则 以满足他们的权威。\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    filtered_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, and high-pass filtering complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:12:46\n",
      "Recognized English: not even in the given\n",
      "Translated Chinese: 即使是在给定的\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Apply silence and noise removal\n",
    "    if show:\n",
    "        print(\"Applying silence and noise removal...\")\n",
    "    clean_audio = filtered_audio.split_to_mono()[0].strip_silence()\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    clean_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, high-pass filtering, and silence/noise removal complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:39<00:00, 1.91MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:15:30\n",
      "Recognized English: In the past we have seen that house, the government has cracked down on this.\n",
      "Translated Chinese: 过去我们见过那所房子,政府就此予以镇压。\n",
      "Recognized English: The properties of those criminals and that is why such things were hard to issue strict guidelines to be alert to by all the state authorities\n",
      "Translated Chinese: 这些罪犯的财产,正因为如此,这类事情很难发布严格的准则,让所有国家当局保持警惕。\n",
      "Recognized English: Fashion League the the municipalities that currently has the crime with endlich.\n",
      "Translated Chinese: 时尚联盟是目前犯罪的地方\n",
      "Recognized English: How they need to take action like you already pointed out that steam and time.\n",
      "Translated Chinese: 他们如何需要采取行动 像你已经指出的 蒸汽和时间。\n",
      "Recognized English: you need to be able to do any individual before you know the demolition takes place to any any for any other household or something of that sort. So the demolition notice, we must be able to make sure that you know the person who is willing to you know the demolition is that part which is illegal and the multiple laws of that particular state can also on their own because if that is what it is the damage you know which is which usually you know is more when the bulldozer arrives than you know tags on the property.\n",
      "Translated Chinese: 您需要能够做任何个人 在您知道拆除发生在 任何其它家庭 或类似的东西之前。 因此,拆除通知, 我们必须能够确保您知道 谁愿意你的人知道 拆除是违法的, 而该特定国家的多重法律 也可以自己做, 因为如果这就是你知道的损害, 通常你知道的 更多的是当推土机到达时 而不是你所知道的 财产上的标签。\n",
      "Recognized English: Let's when an individual who has violated some kind of political law, that is on their own. So the Supreme Court have given a three month window to all the individuals who...\n",
      "Translated Chinese: 让我们来看看谁违反了某种政治法, 也就是他们自己的政治法。所以最高法院已经给所有那些...\n",
      "Recognized English: you know, some kind of the one, some kind of violation, ask for the principal lodge in their house property.\n",
      "Translated Chinese: 你知道, 某种类型的, 某种违反, 要求主宿舍 在他们的房屋内。\n",
      "Recognized English: Do so. Ardu's a pinco-out also because very very clear that there are over all the notices and everything needs to be enough of food.\n",
      "Translated Chinese: 这样做。 Ardu 是一个针刺, 也因为非常清楚, 所有的通知上都有, 所有东西都需要有足够的食物。\n",
      "Recognized English: The state will also be developed by each state. Now, the state will have a huge...\n",
      "Translated Chinese: 州也将由各州发展。 现在,州将有一个巨大的...\n",
      "Recognized English: to develop that process and put out all the notices.\n",
      "Translated Chinese: 发展这一过程,并公布所有通知。\n",
      "Recognized English: properties they are going to take action in next three months and the doctors have been duely issue on us. So everything now after the shipping of guidelines comes.\n",
      "Translated Chinese: 未来三个月,他们将采取行动,医生们已经向我们提出了适当的问题。因此,在准则的运送到来之后,现在的一切事情都发生了。\n",
      "Recognized English: The quoted page\n",
      "Translated Chinese: 引引页\n",
      "Recognized English: Don't WAY\n",
      "Translated Chinese: 不要走路,别走路\n",
      "Recognized English: Go the takeout mids, you'll know,\n",
      "Translated Chinese: 去外卖中间, 你会知道,\n",
      "Recognized English: More and more life will only happen that day from now onwards. But can you correct me if I'm wrong? But essentially the Supreme Court in its verdict has spoken yes about guidelines about how to correct this going forward. But not really about what kind of action should be taken against those who resulted to this previously.\n",
      "Translated Chinese: 只有从现在开始的那一天,生活才会越来越多。但如果我错了,你能纠正我吗?但基本上,最高法院在其判决中肯定地谈到如何纠正这种情况。但是,对于以前导致这种情况的人应该采取什么样的行动,并不真正地说应该采取什么样的行动。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"tiny\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    filtered_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, and high-pass filtering complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:47<00:00, 1.61MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:22:13\n",
      "Recognized English: Yes, actually the guidance will actually be done.\n",
      "Translated Chinese: 是的,实际上,指导将实际完成。\n",
      "Recognized English: about that you know puts to have you know been you know has faced this bulldozer will have to be compensated but in the time of case to case basis now the what is the how this is completely demolished because of the small legality involved in that structure. So, everything needs to be you know measured accordingly that is not as you think would have not laid down anything specifically but yes at least to case basis if all it a person has has a previous problem in you know the how is complete.\n",
      "Translated Chinese: 你知道,你知道,你知道, 面对这辆推土机 将不得不得到补偿, 但在案件审理时, 依据现在的情况, 如何完全拆除它, 因为这个结构涉及的 合法性很小。 所以,每件事都需要你知道 相应的量度, 这不是你认为没有 具体地规定什么,而是是,至少是 案例基础,如果一个人以前有问题, 你知道它是如何完成的。\n",
      "Recognized English: Well, you should go to the minor one.\n",
      "Translated Chinese: 好吧,你应该去 小一个。\n",
      "Recognized English: That particular person can oppose the port again those authorities\n",
      "Translated Chinese: 此人可以再次反对港口当局\n",
      "Recognized English: So, the Supreme Court has clearly set up.\n",
      "Translated Chinese: 因此,最高法院显然已经成立。\n",
      "Recognized English: on it...\n",
      "Translated Chinese: 上... 上... 上... 上... 上...\n",
      "Recognized English: We don't do this.\n",
      "Translated Chinese: 我们不这样做。\n",
      "Recognized English: So\n",
      "Translated Chinese: 苏\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: depends on.\n",
      "Translated Chinese: 视乎情况而定。\n",
      "Recognized English: \"\"\n",
      "Translated Chinese: (\")\"\n",
      "Recognized English: the\n",
      "Translated Chinese: 排\n",
      "Recognized English: I'll get in.\n",
      "Translated Chinese: 我会进去的\n",
      "Recognized English: suction\n",
      "Translated Chinese: 吸管\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: .\n",
      "Translated Chinese: . .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"tiny.en\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    filtered_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, and high-pass filtering complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:23:59\n",
      "Recognized English: I'm Mr. Ganshan Tiwari, your view on this verdict that's coming from the Supreme Court saying very clearly that the state cannot go ahead and arbitrarily res…\n",
      "Translated Chinese: 我是甘山提瓦里先生 你对最高法院的判决的看法 很清楚地说 国家不能任意...\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Let me close the use of the Prime Minister of India, from India to the West side itself, on his victory to Vito. When the Prime Minister nodded, we are just a man, and he said that the nation should learn.\n",
      "Translated Chinese: 请允许我结束印度总理从印度到西方的用武之道,他胜利给维托。 当总理点头时,我们只是一个男人,他说国家应该学习。\n",
      "Recognized English: How good they were, how good they were.\n",
      "Translated Chinese: 他们有多好,多好\n",
      "Recognized English: The same that is in the Code of Bows, the same constitution to people like the time minister and the other staff who have been in power for the kids.\n",
      "Translated Chinese: 同样的,在《鞠躬法》中, 同样的宪法对像时务大臣这样的人 和其他掌权的员工 都一样,他们一直为孩子们服务。\n",
      "Recognized English: And please, for the kids.\n",
      "Translated Chinese: 还有,拜托,为了孩子们\n",
      "Recognized English: I was just saying that the people, the teachers...\n",
      "Translated Chinese: 我只是说, 人民,老师...\n",
      "Recognized English: There's not some in every other week.\n",
      "Translated Chinese: 每隔一周就有一些\n",
      "Recognized English: ruled by constitution.\n",
      "Translated Chinese: 由宪法统治。\n",
      "Recognized English: Und jetzt E mph, dass du das Stück probst.\n",
      "Translated Chinese: E mph, dass du das Stück probst. 斯图克·斯图克·普罗斯特。\n",
      "Recognized English: Totally in vain because they violate the constitution.\n",
      "Translated Chinese: 因为他们违反了宪法,完全徒劳无功。\n",
      "Recognized English: while in the rise of common man, they are used with dictatorial and torturous methods.\n",
      "Translated Chinese: 在普通人的崛起中,他们被使用 独裁和残酷的方法。\n",
      "Recognized English: They've splashed away homes of communities and deep-dive lives, lives and communities.\n",
      "Translated Chinese: 它们喷洒了社区家园 以及深沉的生活、生活和社区\n",
      "Recognized English: So it is a tight slap on the dictatorship of Yogi Adityanath.\n",
      "Translated Chinese: 所以这是对Yogi Adityanath独裁政权的严厉一巴掌。\n",
      "Recognized English: You find that time somebody and you will not learn the lesson that India is a democracy.\n",
      "Translated Chinese: 你找到那个时间 某个人,你不会 吸取教训 印度是一个民主国家。\n",
      "Recognized English: cont seva contGANnys\n",
      "Translated Chinese: 立方公尺 contGANnys\n",
      "Recognized English: The Supreme Court, Mr. Ganshantewari, while speaking about this entire case, has also said, has laid out guidelines, including saying that if it's an illegal structure, you can go ahead and demolish, of course, ensuring that you put across an intimat-\n",
      "Translated Chinese: 最高法院Ghanshantewari先生在谈论整个案件时也说过,他提出了指导方针,包括说如果这是一个非法结构,你可以着手拆除,当然,确保你贴上一个暗处,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    filtered_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, and high-pass filtering complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:27:54\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Now the yogi government as have several other state governments that have resorted to this bulldozer justice model always maintain that the property is an illegal structure.\n",
      "Translated Chinese: 现在瑜伽政府和其他几个采用推土机司法模式的州政府一样,总是认为该财产是非法结构。\n",
      "Recognized English: They can see what they want.\n",
      "Translated Chinese: 他们可以看到他们想要什么。\n",
      "Recognized English: We leave that in the...\n",
      "Translated Chinese: 我们把那个留在...\n",
      "Recognized English: The idea of cutting it or cutting it, they believe minorities are not legal entities in this country.\n",
      "Translated Chinese: 他们认为少数民族不是这个国家的法律实体。\n",
      "Recognized English: Dalits are not living entities in this country.\n",
      "Translated Chinese: 达利特人不是生活在这个国家的实体。\n",
      "Recognized English: They ran against the farmers, the biggest farmers, and were not legal entities.\n",
      "Translated Chinese: 他们与农民竞争,而农民是最大的农民,他们不是法律实体。\n",
      "Recognized English: The idea of the dictatorship is not the idea of India. The Supreme Court is teaching this every other week to Modi government and Modi government.\n",
      "Translated Chinese: 独裁思想不是印度的理念。 最高法院每隔一个星期就向莫迪政府和莫迪政府传授这一理念。\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Mr. Ganshan Thibari, thank you for joining us with your view on this verdict that's coming in from the Supreme Court. I'd like to bring in our legal editor, Nalini Sharma, also for more and to really decode what the top court has said. Nalini, very, very scathing comments coming in from the Supreme Court on this kind of bulldozer justice that's become very popular in many, many states. And they've made it very clear with the guidelines laid out that there must be a notice issue prior to anyone and simply because they're accused of a crime, they're convicted in a crime, that's not reason enough to target their properties.\n",
      "Translated Chinese: Ganshan Thibari先生, 感谢你和我们一起看待最高法院即将做出的判决。 我想把法律编辑Nalini Sharma也带入我们的法律编辑Nalini Sharma, 并且真正地破解了最高法院所说的话。Nalini, 最高法院在这种推土机司法方面提出的非常非常非常令人吃惊的评论, 在许多州,很多州,这种推土机司法已经变得非常流行。他们非常明确地指出,在任何人之前必须有一个通知问题, 仅仅因为他们被指控犯罪, 他们被判有罪, 这不足以成为攻击其财产的理由。\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Now what's interesting is that we've seen several times in the past Supreme Court come forth with this order that houses or properties of any person even if they're in convict cannot be demolished. But what's happened today is that the Supreme Court has exercised its special powers under Article 142 of the Constitution that allows the Supreme Court to be able to do anything to protect the people of the country. So, what's interesting is that we've seen several times in the past Supreme Court come forth with this order that houses or properties of any person even if they're in convict cannot be demolished.\n",
      "Translated Chinese: 有趣的是,我们在过去的最高法院中见过好几次这样的命令:任何人的房屋或财产,即使他们已被定罪,也不能被拆除。但今天发生的情况是,最高法院行使了《宪法》第142条规定的特别权力,使最高法院能够做任何事情来保护国家人民。因此,有趣的是,我们在过去的最高法院中见过好几次这样的命令:任何人的房屋或财产,即使他们已被定罪,也不能被拆除。\n",
      "Recognized English: To pass any order to do complete justice in a case. So under Article 142 of the Constitution are these directions that have been issued by the Supreme Court and let me quickly take you through the guidelines that the Supreme Court has issued today. First of all, the Supreme Court has said that demolition of a property has to be the last resort that must be resorted.\n",
      "Translated Chinese: 因此,根据《宪法》第142条,最高法院下达了这些指示,让我很快地告诉你最高法院今天颁布的准则。 首先,最高法院说,拆除财产必须作为最后手段。\n",
      "Recognized English: Even when a notice for the demolition has been given, it is important that the owner of the property is given time not only to appear before the authorities and put a cross-point of view, but he should also be given time to challenge that notice before a co-\n",
      "Translated Chinese: 即使已经发出拆除通知,重要的是,财产所有人不仅要有时间在当局面前露面,提出交叉观点,而且还要有时间在共同当事人面前质疑该通知。\n",
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n",
      "Recognized English: Thanks for watching!\n",
      "Translated Chinese: 感谢观看!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: Thank you.\n",
      "Translated Chinese: 谢谢\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    filtered_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, and high-pass filtering complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:30:33\n",
      "Recognized English: Put the flow in case there is anything wrong in the motor that is been issued by the executive. The code is also gone ahead and said that this amounts to collective punishment. When there is only one person who is accused of convicted of a crime and a house is demolished or the property is demolished, it only doesn't just affect him, it also affects his family members. So this amounts to collective punishment which cannot be allowed under the law, he being in mind that right to shelter is a fundamental right of all citizens. In fact, the Supreme Court has gone ahead to see that it is really sad to see that women and children are being thrown out of their properties through this arbitrary abuse of process and the abuse of laws that the executive has been doing. The Supreme Court also went ahead and said that the notice that has been put or served to our owner for the knowledge of their properties should be prominently displayed on the properties and while the demolition is being carried out, there should be a video graph of the entire incident including the officers who were present during this demolition. While the Supreme Court said this is because in case this was an arbitrary or illegal action or it was done without due process, then those officials need to be made as responsible and they need to be made responsible for restitution of the property which means that they need to ensure that property if illegal and abolished has to be given back to the owner.\n",
      "Translated Chinese: 将流动情况放在执行官签发的马达中出现任何错误的情况下, 这部法典也向前发展, 并说这相当于集体惩罚。 当只有一人被指控犯有罪行,房屋被拆毁或财产被拆毁时, 这不仅仅影响到他, 也影响到他的家庭成员。 因此, 这相当于集体惩罚, 根据法律不允许, 拆除权是全体公民的一项基本权利。 事实上, 最高法院已经看到, 妇女和儿童由于任意滥用程序或滥用行政官一直在从事的法律而被迫离开其财产, 这实在是令人悲哀的。 最高法院还提前指出, 已经或已经通知我们所有者了解其财产的通知应该突出地显示在财产上, 在拆除过程中, 应该有整个事件的视频图, 包括这次拆除期间在场的官员。 最高法院说, 这是因为,如果这是任意或非法行动,或者没有经过正当程序,就已经发生了这种行为。 最高法院还说,如果那些官员需要为归还财产而承担责任,那么他们就必须承担责任。\n",
      "Recognized English: Same with me, I'd like to also bring you Anisha, a mother who is in the Supreme Court in Mendes Herring was also taking place. Anisha, just explained to us with regards to these guidelines.\n",
      "Translated Chinese: 我同样也想带你们去Anisha, 一位母亲在Mendes Herring的最高法院工作。Anisha,刚刚向我们解释了这些指导方针。\n",
      "Recognized English: case of an illegal structure being demolished.\n",
      "Translated Chinese: 非法建筑被拆除的案件。\n",
      "Recognized English: guidelines not then you know hold to then not have to follow the Supreme Court guidelines.\n",
      "Translated Chinese: 并非指针指针指针指针指针指针指针指针,而无需遵循最高法院的指针。\n",
      "Recognized English: But, the court has made a very very clear or there will be any annual demolition.\n",
      "Translated Chinese: 但是,法院已经非常明确地指出,否则每年将发生任何拆除行动。\n",
      "Recognized English: to follow these guidelines.\n",
      "Translated Chinese: 遵守本指南。\n",
      "Recognized English: Even if a structure is even if a structure is on the public road or a railway line or\n",
      "Translated Chinese: 即使一个结构即使位于公共公路或铁路线上,或\n",
      "Recognized English: What even then the notice has to be given, the court has clearly said that any and all the demolitions must follow the process first upon this.\n",
      "Translated Chinese: 即使当时必须发出通知,法院也明确表示,任何和所有拆除都必须首先遵循拆除进程。\n",
      "Recognized English: not the structure is actually unauthorized.\n",
      "Translated Chinese: 而不是这个结构实际上是未经授权的。\n",
      "Recognized English: is there any compounding of that illegality that is possible because we seen in the case of Delhi's specially and several other states, if you may as a for example make an extension of a balcony in your house then you can always demolish just the balcony you can pay for compounding. So the court said those aspects must be taken into account.\n",
      "Translated Chinese: 这种非法性可能加剧,因为我们在德里和其他几个州的情况中看到,举例来说,如果你可以把房子的阳台扩建到一个阳台,那么你总是可以拆除你付得起加固费的阳台。因此,法院说,这些方面必须加以考虑。\n",
      "Recognized English: My colleague was just explaining any demolition that has to be done first. Notice has to go to the owner of the property via registered pub.\n",
      "Translated Chinese: 我的同事只是在解释任何必须首先拆除的房屋。 必须通过注册的酒吧通知房产所有人。\n",
      "Recognized English: to a male but also has to be tested.\n",
      "Translated Chinese: 对男性,但也必须接受测试。\n",
      "Recognized English: in a place near that particular property. Secondly, that notice has to go also to the district magistrate's office to ensure that there is no illegal backstating of the show cause notices. 15 days must be given to the occupy of the property to either challenge the show cause notice or it's possible to compound or remove that illegality to take the steps to do so. The court has not been very, very clear that gold also adjusts to the piece seen in\n",
      "Translated Chinese: 其次,该通知必须同时送交地区地方治安法官办公室,以确保不发生非法退回显示理由通知的情况。 必须对财产的占有给予15天的时间,以对显示理由通知提出质疑,或者有可能使该非法性复杂化或取消,以便采取步骤这样做。\n",
      "Recognized English: Concrete in the last few years is patently illegal it is unconstitutional that is\n",
      "Translated Chinese: 过去几年的混凝土显然是非法的,这是违宪的,\n",
      "Recognized English: one can be deprived of their house or even commercial property.\n",
      "Translated Chinese: 任何人可被剥夺其房屋或甚至商业财产。\n",
      "Recognized English: three and eggs\n",
      "Translated Chinese: 三个和三个鸡蛋\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 50\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39msamplerate, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Read 0.1-second chunks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         audio_data\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Calculate the energy of the current frame\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1475\u001b[0m, in \u001b[0;36mInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1473\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m   1474\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[0;32m-> 1475\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1245\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[0;34m(self, frames)\u001b[0m\n\u001b[1;32m   1243\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[1;32m   1244\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[0;32m-> 1245\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[1;32m   1247\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"tiny.en\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    filtered_audio.export(filename, format=\"wav\")\n",
    "    \n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, and high-pass filtering complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started at 2024-11-14 00:46:51\n",
      "Recognized English: Check it out.\n",
      "Translated Chinese: 检查出来。\n",
      "Recognized English: Let's bring in our own this broadcast now which is the party of the Congress for an interview.\n",
      "Translated Chinese: 让我们把现在的这个节目 带进来,这是国会的党 接受采访。\n",
      "Recognized English: I\n",
      "Translated Chinese: 一一\n",
      "Recognized English: What do you have to say about the Supreme Court's verdict making it very very clear that this bulldozer action is unacceptable and is a breach of fundamental rights.\n",
      "Translated Chinese: 关于最高法院的判决,你有什么话要说? 说得很清楚,这种推土机行动是不可接受的,是对基本权利的侵犯。\n",
      "Recognized English: .\n",
      "Translated Chinese: . .\n",
      "Recognized English: So, from what is the very good decision.\n",
      "Translated Chinese: 所以,从什么是非常好的决定。\n",
      "Recognized English: This cultural pipeline, to learn from area\n",
      "Translated Chinese: 这条文化管道,从地区中学习\n",
      "Recognized English: Cheers Guys ! I Ri\n",
      "Translated Chinese: 干杯,伙计们!\n",
      "Recognized English: Their\n",
      "Translated Chinese: 它们的\n",
      "Recognized English: And the package expenses enhancement to Lufan\n",
      "Translated Chinese: 以及增加卢凡的一揽子开支\n",
      "Recognized English: a, b, ane usual term chinosaur Fe, c, b, k,I'prod seen g,i k, b to d. So therefore it's succession, go\n",
      "Translated Chinese: a, b, 通常的中下风鱼 fe, c, b, k, k, I'prod seen g, i k, b to d. 因此它是继承, go\n",
      "Recognized English: utensist followers can continue to mhide back as a political denomination. w head compensationemi proof there were at least no account chsh. machin s work\n",
      "Translated Chinese: 独裁者追随者可以作为一个政治派别继续回归。\n",
      "Recognized English: Oh, in legal structure is only one thing, political agent or country shall hear.\n",
      "Translated Chinese: 在法律结构中,政治代理人或国家只听到一件事\n",
      "Recognized English: We can't take this back for that show but you can also buy wealth in $28.\n",
      "Translated Chinese: 我们不能拿这个回去看那个节目 但你也可以在28美元里买到财富\n",
      "Recognized English: Thank you very much.\n",
      "Translated Chinese: 非常感谢。\n",
      "Recognized English: Have a great day. Thank you.\n",
      "Translated Chinese: 祝你今天愉快 谢谢\n",
      "Recognized English: Balaqez political aircraft, trump arpeyes and saw apartheid and Taillaten T there was\n",
      "Translated Chinese: 巴拉盖兹政治飞机、阿尔贝耶斯王、看到种族隔离和Taillaten T\n",
      "Recognized English: laughing and taking\n",
      "Translated Chinese: 笑笑,并取笑,\n",
      "Recognized English: She's a part of our journey. As you all are in your today with your view on this big judgment that's coming from the Supreme Court. We've been seeing this kind of bulldozers going ahead and targeting homes, properties of accused in Uttar Pradesh, in Madhya Pradesh and referred to as the Muldhosa module of justice. But a question, the opposition constantly is asking, Nalani and Yifu Kottro Mo light on that is what action is the Supreme Court actually talking about on those who resulted to this kind of bulldozer action previously? Have they actually spoken about some action that officials will face or authorities will face?\n",
      "Translated Chinese: 她是我们的旅程的一部分。 正如你今天所看到的一样, 您对最高法院的这一重大判决的看法。 我们一直看到这种推土机继续前进, 瞄准北方邦、中央邦和中央邦的房屋, 被告的财产, 被称为Muldhosa司法单元。 但一个问题, 反对派不断问, 纳拉尼 和 Yifu Kottro Mo 指出, 最高法院究竟在谈论什么行动呢? 他们是否真的谈到官员或当局会面对的某种行动?\n",
      "Recognized English: What I got to do is inting\n",
      "Translated Chinese: 我要做的就是忍着\n",
      "Recognized English: in\n",
      "Translated Chinese: 内\n",
      "Recognized English: another judge kind of\n",
      "Translated Chinese: 或别的法官,\n",
      "Recognized English: To record a strategy quantum select item\n",
      "Translated Chinese: 记录战略量数选择项目\n",
      "Recognized English: We will Transformers doors.\n",
      "Translated Chinese: 我们将变形门。\n",
      "Recognized English: I'm going to hang in.\n",
      "Translated Chinese: 我会坚持下去的\n",
      "Recognized English: Thank you!\n",
      "Translated Chinese: 谢谢!\n",
      "Recognized English: cross it.\n",
      "Translated Chinese: 穿过它。\n",
      "Recognized English: Which means...\n",
      "Translated Chinese: 这意味着...\n",
      "Recognized English: Any\n",
      "Translated Chinese: 任何\n",
      "Recognized English: Don't look at that.\n",
      "Translated Chinese: 别看那个\n",
      "Recognized English: Label\n",
      "Translated Chinese: 标签标签标签\n",
      "Recognized English: any good\n",
      "Translated Chinese: 任何好\n",
      "Recognized English: This is your view.\n",
      "Translated Chinese: 这是你的观点\n",
      "Recognized English: This is...\n",
      "Translated Chinese: 这是...\n",
      "Recognized English: L\n",
      "Translated Chinese: L L L\n",
      "Recognized English: Access\n",
      "Translated Chinese: 获得机会\n",
      "Recognized English: I hope it!\n",
      "Translated Chinese: 我希望它!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 158\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Continuously record audio until stopped\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Record audio and save to a temporary file\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[43mrecord_audio_until_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 72\u001b[0m, in \u001b[0;36mrecord_audio_until_silence\u001b[0;34m(filename, samplerate)\u001b[0m\n\u001b[1;32m     69\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     70\u001b[0m silence_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 72\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInputStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Read 0.1-second chunks\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1111\u001b[0m, in \u001b[0;36m_StreamBase.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stop and close the stream when exiting a \"with\" statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sounddevice.py:1137\u001b[0m, in \u001b[0;36m_StreamBase.stop\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop\u001b[39m(\u001b[38;5;28mself\u001b[39m, ignore_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Terminate audio processing.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m    This waits until all pending audio buffers have been played\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_StopStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_errors:\n\u001b[1;32m   1139\u001b[0m         _check(err, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError stopping stream\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize\n",
    "\n",
    "# Switch to show recording status\n",
    "show = 0\n",
    "\n",
    "if not show:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize Whisper model (choose a smaller model like \"base\" or \"small\" for faster performance)\n",
    "model = whisper.load_model(\"tiny.en\")\n",
    "\n",
    "# Initialize MarianMT model and tokenizer for offline translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Parameters for audio recording\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000  # Adjust this threshold based on your environment\n",
    "silence_duration = 0.5  # Duration (in seconds) of silence to stop recording\n",
    "\n",
    "# Queue to hold audio files to be processed\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Path for the combined script file, name file with current_time saved when start listening\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# Variable to store the last processed English text\n",
    "last_processed_text = None\n",
    "\n",
    "# Function to apply dynamic range compression and gain control to enhance speech\n",
    "def enhance_speech(filename):\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "\n",
    "    # Apply dynamic range compression to reduce the difference between soft and loud sounds\n",
    "    if show:\n",
    "        print(\"Applying dynamic range compression...\")\n",
    "    compressed_audio = audio_segment.compress_dynamic_range(threshold=-20.0, ratio=4.0, attack=5.0, release=50.0)\n",
    "\n",
    "    # Apply gain control to amplify speech if it's too quiet\n",
    "    if show:\n",
    "        print(\"Applying gain control...\")\n",
    "    amplified_audio = compressed_audio + 5  # Increase volume by 5 dB (you can adjust this)\n",
    "\n",
    "    # Normalize the audio to ensure the overall loudness is balanced\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = amplified_audio.normalize()\n",
    "\n",
    "    # Export the enhanced audio\n",
    "    normalized_audio.export(filename, format=\"wav\")\n",
    "    if show:\n",
    "        print(\"Speech enhancement (compression and gain control) complete!\")\n",
    "\n",
    "# Function to record audio until silence is detected\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    if show:\n",
    "        print(\"Recording...\")\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    \n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while True:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "            audio_data.append(frame)\n",
    "            \n",
    "            # Calculate the energy of the current frame\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "\n",
    "            # If silence has been detected for a sufficient duration, stop recording\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    \n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    \n",
    "    # Apply noise reduction using pydub\n",
    "    if show:\n",
    "        print(\"Applying noise reduction...\")\n",
    "    audio_segment = AudioSegment.from_wav(filename)\n",
    "    reduced_noise_audio = audio_segment - 10  # Simple noise reduction\n",
    "\n",
    "    # Apply normalization\n",
    "    if show:\n",
    "        print(\"Normalizing audio...\")\n",
    "    normalized_audio = reduced_noise_audio.normalize()\n",
    "\n",
    "    # Apply high-pass filter\n",
    "    if show:\n",
    "        print(\"Applying high-pass filter...\")\n",
    "    filtered_audio = normalized_audio.high_pass_filter(300)  # 300 Hz cutoff frequency\n",
    "\n",
    "    # Export the processed audio back to a file\n",
    "    filtered_audio.export(filename, format=\"wav\")\n",
    "\n",
    "    # Apply speech enhancement (dynamic range compression and gain control)\n",
    "    enhance_speech(filename)\n",
    "\n",
    "    if show:\n",
    "        print(\"Recording, noise reduction, normalization, high-pass filtering, and enhancement complete!\")\n",
    "    audio_queue.put(filename)  # Add the recorded audio to the processing queue\n",
    "\n",
    "# Function to translate English text to Chinese using the MarianMT model\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Function to process audio (transcribe and translate)\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()  # Get the next audio file from the queue\n",
    "        if filename:\n",
    "            # Transcribe using Whisper\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            \n",
    "            # Only process if the new text is different from the last processed text\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                print(\"Recognized English:\", english_text)\n",
    "                last_processed_text = english_text  # Update the last processed text\n",
    "\n",
    "                # Translate to Chinese\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                print(\"Translated Chinese:\", chinese_text)\n",
    "\n",
    "                # Write both English and Chinese to the combined script file\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(\"English: \" + english_text + \"\\n\")\n",
    "                    combined_file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                    combined_file.write(\"\\n\")  # Add a blank line for separation\n",
    "        \n",
    "        audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"Recording started at\", current_time)\n",
    "# Start a thread for continuous audio processing\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "# Continuously record audio until stopped\n",
    "while True:\n",
    "    # Record audio and save to a temporary file\n",
    "    record_audio_until_silence(\"temp_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "录音开始，请说话...\n",
      "录音结束。\n",
      "音频已保存为 recorded_audio.wav\n"
     ]
    },
    {
     "ename": "InvalidDuration",
     "evalue": "padding cannot be longer than silence_len",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidDuration\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m record_audio()\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 处理录制的音频\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43menhance_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FILENAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menhanced_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 64\u001b[0m, in \u001b[0;36menhance_audio\u001b[0;34m(input_filename, output_filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m silence_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m  \u001b[38;5;66;03m# 静音阈值，单位为 dBFS\u001b[39;00m\n\u001b[1;32m     62\u001b[0m chunk_length_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# 检测静音的时间窗口，单位为毫秒\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m trimmed_audio \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_audio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msilence_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilence_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_length_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 导出处理后的音频\u001b[39;00m\n\u001b[1;32m     67\u001b[0m trimmed_audio\u001b[38;5;241m.\u001b[39mexport(output_filename, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/site-packages/pydub/effects.py:100\u001b[0m, in \u001b[0;36mstrip_silence\u001b[0;34m(seg, silence_len, silence_thresh, padding)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@register_pydub_effect\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstrip_silence\u001b[39m(seg, silence_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, silence_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padding \u001b[38;5;241m>\u001b[39m silence_len:\n\u001b[0;32m--> 100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidDuration(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding cannot be longer than silence_len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m split_on_silence(seg, silence_len, silence_thresh, padding)\n\u001b[1;32m    103\u001b[0m     crossfade \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mInvalidDuration\u001b[0m: padding cannot be longer than silence_len"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "from pydub.effects import normalize, high_pass_filter\n",
    "\n",
    "# 录音参数\n",
    "FORMAT = pyaudio.paInt16  # 16位深度\n",
    "CHANNELS = 1              # 单声道\n",
    "RATE = 16000              # 采样率\n",
    "CHUNK = 1024              # 每个数据块的大小\n",
    "RECORD_SECONDS = 10       # 录音时间（可以自定义）\n",
    "OUTPUT_FILENAME = \"recorded_audio.wav\"  # 输出文件名\n",
    "\n",
    "def record_audio():\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    \n",
    "    print(\"录音开始，请说话...\")\n",
    "    frames = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "    except KeyboardInterrupt:\n",
    "        # 按 Ctrl+C 停止录音\n",
    "        print(\"录音结束。\")\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "    # 保存录制的音频\n",
    "    wf = wave.open(OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "    print(f\"音频已保存为 {OUTPUT_FILENAME}\")\n",
    "\n",
    "def enhance_audio(input_filename, output_filename):\n",
    "    # 加载音频文件\n",
    "    audio = AudioSegment.from_file(input_filename, format=\"wav\")\n",
    "\n",
    "    # 增加音量\n",
    "    louder_audio = audio + 10  # 增加10 dB音量\n",
    "\n",
    "    # 归一化音频\n",
    "    normalized_audio = normalize(louder_audio)\n",
    "\n",
    "    # 高通滤波去除低频噪音\n",
    "    filtered_audio = high_pass_filter(normalized_audio, cutoff=300)  # 300 Hz截止频率\n",
    "\n",
    "    # 去除静音部分\n",
    "    silence_threshold = -40  # 静音阈值，单位为 dBFS\n",
    "    chunk_length_ms = 10  # 检测静音的时间窗口，单位为毫秒\n",
    "\n",
    "    trimmed_audio = filtered_audio.strip_silence(silence_thresh=silence_threshold, silence_len=chunk_length_ms)\n",
    "\n",
    "    # 导出处理后的音频\n",
    "    trimmed_audio.export(output_filename, format=\"wav\")\n",
    "    print(f\"处理后的音频已保存为 {output_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 录音\n",
    "    record_audio()\n",
    "\n",
    "    # 处理录制的音频\n",
    "    enhance_audio(OUTPUT_FILENAME, \"enhanced_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "strip_silence() got an unexpected keyword argument 'silence_chunk_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menhance_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecorded_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menhanced_audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 64\u001b[0m, in \u001b[0;36menhance_audio\u001b[0;34m(input_filename, output_filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m silence_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m40\u001b[39m  \u001b[38;5;66;03m# 静音阈值，单位为 dBFS\u001b[39;00m\n\u001b[1;32m     62\u001b[0m chunk_length_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# 检测静音的时间窗口，单位为毫秒\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m trimmed_audio \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_audio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip_silence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msilence_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilence_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence_chunk_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_length_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 导出处理后的音频\u001b[39;00m\n\u001b[1;32m     67\u001b[0m trimmed_audio\u001b[38;5;241m.\u001b[39mexport(output_filename, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: strip_silence() got an unexpected keyword argument 'silence_chunk_len'"
     ]
    }
   ],
   "source": [
    "enhance_audio(\"recorded_audio.wav\", \"enhanced_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在录音...按 Ctrl+C 停止\n",
      "保存音频片段: recording_part_1.wav\n",
      "保存音频片段: recording_part_2.wav\n",
      "保存音频片段: recording_part_3.wav\n",
      "保存音频片段: recording_part_4.wav\n",
      "保存音频片段: recording_part_5.wav\n",
      "保存音频片段: recording_part_6.wav\n",
      "保存音频片段: recording_part_7.wav\n",
      "保存音频片段: recording_part_8.wav\n",
      "录音结束\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import datetime\n",
    "\n",
    "# 录音参数\n",
    "RATE = 16000          # 采样率\n",
    "CHUNK = 1024          # 每个数据块大小\n",
    "SILENCE_THRESHOLD = 0.01  # 静音检测阈值\n",
    "MIN_SILENCE_LEN = 5   # 静音最小持续时间（秒）\n",
    "\n",
    "def detect_silence(data_chunk, threshold):\n",
    "    \"\"\"检测音频数据是否为静音\"\"\"\n",
    "    volume = np.sqrt(np.mean(data_chunk**2))  # 计算RMS音量\n",
    "    return volume < threshold\n",
    "\n",
    "def record_and_split():\n",
    "    recording = []\n",
    "    silent_chunks = 0\n",
    "    part_number = 1\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        nonlocal recording, silent_chunks, part_number\n",
    "\n",
    "        if status:\n",
    "            print(status)\n",
    "\n",
    "        # 检测是否为静音\n",
    "        is_silent = detect_silence(indata, SILENCE_THRESHOLD)\n",
    "\n",
    "        if is_silent:\n",
    "            silent_chunks += 1\n",
    "        else:\n",
    "            silent_chunks = 0\n",
    "\n",
    "        # 记录音频数据\n",
    "        recording.extend(indata.copy())\n",
    "\n",
    "        # 如果静音持续超过MIN_SILENCE_LEN秒，保存当前录音\n",
    "        if silent_chunks > (RATE / CHUNK * MIN_SILENCE_LEN):\n",
    "            if len(recording) > 0:\n",
    "                filename = f\"recording_part_{part_number}.wav\"\n",
    "                wavio.write(filename, np.array(recording), RATE, sampwidth=2)\n",
    "                print(f\"保存音频片段: {filename}\")\n",
    "                part_number += 1\n",
    "                recording = []\n",
    "                silent_chunks = 0\n",
    "\n",
    "    # 开始录音\n",
    "    with sd.InputStream(callback=callback, channels=1, samplerate=RATE, blocksize=CHUNK):\n",
    "        print(\"正在录音...按 Ctrl+C 停止\")\n",
    "        try:\n",
    "            while True:\n",
    "                pass  # 主循环保持运行\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"录音结束\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    record_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started. Press Ctrl+C to stop.\n",
      "Recognized English: Hello everybody.\n",
      "Translated Chinese: 大家好,你们好\n",
      "Recognized English: And again, welcome to Foundations to Algor-\n",
      "Translated Chinese: 欢迎来到阿尔戈尔基金会\n",
      "Recognized English: The last session of...\n",
      "Translated Chinese: 最后一次会议...\n",
      "Recognized English: That's what we're meant to-\n",
      "Translated Chinese: 这就是我们想要...\n",
      "Recognized English: Oh boy.\n",
      "Translated Chinese: 哦,男孩。 呵呵,男孩。\n",
      "Recognized English: ...\n",
      "Translated Chinese: .\n",
      "\n",
      "Stopping recording...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized English: Four…\n",
      "Translated Chinese: 四个... Four...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "SAMPLERATE = 16000\n",
    "ENERGY_THRESHOLD = 1000  # Threshold for detecting silence\n",
    "SILENCE_DURATION = 0.5  # Duration of silence to stop recording\n",
    "SHOW_STATUS = 0  # Set to 1 to display recording/transcription progress\n",
    "COMBINED_SCRIPT_PATH = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self, samplerate=SAMPLERATE, energy_threshold=ENERGY_THRESHOLD, silence_duration=SILENCE_DURATION):\n",
    "        self.samplerate = samplerate\n",
    "        self.energy_threshold = energy_threshold\n",
    "        self.silence_duration = silence_duration\n",
    "\n",
    "    def record_until_silence(self, filename):\n",
    "        if SHOW_STATUS:\n",
    "            print(\"Recording...\")\n",
    "        audio_data = []\n",
    "        silence_counter = 0\n",
    "\n",
    "        with sd.InputStream(samplerate=self.samplerate, channels=1, dtype='int16') as stream:\n",
    "            while True:\n",
    "                frame = stream.read(int(self.samplerate * 0.1))[0]  # Read 0.1-second chunks\n",
    "                audio_data.append(frame)\n",
    "\n",
    "                # Calculate the energy of the current frame\n",
    "                if np.max(np.abs(frame)) < self.energy_threshold:\n",
    "                    silence_counter += 0.1\n",
    "                else:\n",
    "                    silence_counter = 0\n",
    "\n",
    "                # If silence is detected for a sufficient duration, stop recording\n",
    "                if silence_counter >= self.silence_duration:\n",
    "                    break\n",
    "\n",
    "        audio_data = np.concatenate(audio_data, axis=0)\n",
    "        wavio.write(filename, audio_data, self.samplerate, sampwidth=2)\n",
    "        if SHOW_STATUS:\n",
    "            print(\"Recording complete!\")\n",
    "        return filename\n",
    "\n",
    "\n",
    "class WhisperTranscriber:\n",
    "    def __init__(self, model_size=\"small\"):\n",
    "        self.model = whisper.load_model(model_size)\n",
    "\n",
    "    def transcribe(self, audio_file):\n",
    "        result = self.model.transcribe(audio_file, fp16=False)\n",
    "        text = result[\"text\"].strip()\n",
    "        # if SHOW_STATUS:\n",
    "        # if text and text != last_processed_text:\n",
    "        if text:\n",
    "            print(\"Recognized English:\", text)\n",
    "        return text\n",
    "\n",
    "\n",
    "class MarianTranslator:\n",
    "    def __init__(self, model_name='Helsinki-NLP/opus-mt-en-zh'):\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        self.model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "    def translate(self, text):\n",
    "        inputs = self.tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "        translated = self.model.generate(**inputs)\n",
    "        translated_text = self.tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "        # if SHOW_STATUS:\n",
    "        print(\"Translated Chinese:\", translated_text)\n",
    "        return translated_text\n",
    "\n",
    "\n",
    "class AudioProcessor:\n",
    "    def __init__(self, recorder, transcriber, translator, script_path):\n",
    "        self.recorder = recorder\n",
    "        self.transcriber = transcriber\n",
    "        self.translator = translator\n",
    "        self.script_path = script_path\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.last_processed_text = None\n",
    "\n",
    "    def process_audio(self):\n",
    "        while True:\n",
    "            filename = self.audio_queue.get()  # Get the next audio file from the queue\n",
    "            if filename:\n",
    "                # Transcribe the audio\n",
    "                english_text = self.transcriber.transcribe(filename)\n",
    "\n",
    "                # Process only if the text is different from the last processed\n",
    "                if english_text and english_text != self.last_processed_text:\n",
    "                    self.last_processed_text = english_text\n",
    "\n",
    "                    # Translate to Chinese\n",
    "                    chinese_text = self.translator.translate(english_text)\n",
    "\n",
    "                    # Write to script file\n",
    "                    with open(self.script_path, \"a\") as file:\n",
    "                        file.write(\"English: \" + english_text + \"\\n\")\n",
    "                        file.write(\"Chinese: \" + chinese_text + \"\\n\")\n",
    "                        file.write(\"\\n\")  # Add a blank line for separation\n",
    "\n",
    "            self.audio_queue.task_done()  # Mark the task as done\n",
    "\n",
    "    def add_audio_to_queue(self, filename):\n",
    "        self.audio_queue.put(filename)\n",
    "\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize components\n",
    "    recorder = AudioRecorder()\n",
    "    transcriber = WhisperTranscriber()\n",
    "    translator = MarianTranslator()\n",
    "    processor = AudioProcessor(recorder, transcriber, translator, COMBINED_SCRIPT_PATH)\n",
    "\n",
    "    # Start the processing thread\n",
    "    threading.Thread(target=processor.process_audio, daemon=True).start()\n",
    "\n",
    "    print(\"Recording started. Press Ctrl+C to stop.\")\n",
    "    while True:\n",
    "        try:\n",
    "            # Record audio and add it to the processing queue\n",
    "            temp_filename = \"temp_audio.wav\"\n",
    "            recorded_file = recorder.record_until_silence(temp_filename)\n",
    "            processor.add_audio_to_queue(recorded_file)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping recording...\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 08:51:15.018 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.148 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/wangcan/.pyenv/versions/3.12.2/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-11-20 08:51:15.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.150 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.153 Session state does not function when running a script without `streamlit run`\n",
      "2024-11-20 08:51:15.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.165 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.165 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.167 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.167 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-20 08:51:15.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import threading\n",
    "import queue\n",
    "import datetime\n",
    "import streamlit as st\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 初始化模型\n",
    "model = whisper.load_model(\"small\")\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-zh'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "translation_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# 参数\n",
    "samplerate = 16000\n",
    "energy_threshold = 1000\n",
    "silence_duration = 0.5\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# 保存转录和翻译的文本文件\n",
    "combined_script_path = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \".txt\"\n",
    "\n",
    "# 状态变量\n",
    "is_recording = False\n",
    "last_processed_text = None\n",
    "\n",
    "# 音频录制函数\n",
    "def record_audio_until_silence(filename, samplerate=16000):\n",
    "    audio_data = []\n",
    "    silence_counter = 0\n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, dtype='int16') as stream:\n",
    "        while is_recording:\n",
    "            frame = stream.read(int(samplerate * 0.1))[0]\n",
    "            audio_data.append(frame)\n",
    "            if np.max(np.abs(frame)) < energy_threshold:\n",
    "                silence_counter += 0.1\n",
    "            else:\n",
    "                silence_counter = 0\n",
    "            if silence_counter >= silence_duration:\n",
    "                break\n",
    "    audio_data = np.concatenate(audio_data, axis=0)\n",
    "    wavio.write(filename, audio_data, samplerate, sampwidth=2)\n",
    "    audio_queue.put(filename)\n",
    "\n",
    "# 翻译函数\n",
    "def translate_to_chinese(text):\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
    "    translated = translation_model.generate(**inputs)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "# 音频处理函数\n",
    "def process_audio():\n",
    "    global last_processed_text\n",
    "    while True:\n",
    "        filename = audio_queue.get()\n",
    "        if filename:\n",
    "            result = model.transcribe(filename, fp16=False)\n",
    "            english_text = result[\"text\"].strip()\n",
    "            if english_text and english_text != last_processed_text:\n",
    "                last_processed_text = english_text\n",
    "                chinese_text = translate_to_chinese(english_text)\n",
    "                with open(combined_script_path, \"a\") as combined_file:\n",
    "                    combined_file.write(f\"English: {english_text}\\n\")\n",
    "                    combined_file.write(f\"Chinese: {chinese_text}\\n\\n\")\n",
    "                st.session_state[\"transcription\"] = english_text\n",
    "                st.session_state[\"translation\"] = chinese_text\n",
    "        audio_queue.task_done()\n",
    "\n",
    "# Streamlit 应用\n",
    "st.title(\"音频录制与实时翻译\")\n",
    "st.write(\"点击下方按钮开始或停止录音，并查看实时的转录和翻译结果。\")\n",
    "\n",
    "# 状态初始化\n",
    "if \"transcription\" not in st.session_state:\n",
    "    st.session_state[\"transcription\"] = \"\"\n",
    "if \"translation\" not in st.session_state:\n",
    "    st.session_state[\"translation\"] = \"\"\n",
    "\n",
    "# 按钮控制\n",
    "if st.button(\"开始录音\"):\n",
    "    if not is_recording:\n",
    "        is_recording = True\n",
    "        st.write(\"录音中...\")\n",
    "        threading.Thread(target=record_audio_until_silence, args=(\"temp_audio.wav\",), daemon=True).start()\n",
    "        threading.Thread(target=process_audio, daemon=True).start()\n",
    "else:\n",
    "    is_recording = False\n",
    "    st.write(\"录音已停止。\")\n",
    "\n",
    "# 显示结果\n",
    "st.subheader(\"转录结果 (英文)\")\n",
    "st.write(st.session_state[\"transcription\"])\n",
    "\n",
    "st.subheader(\"翻译结果 (中文)\")\n",
    "st.write(st.session_state[\"translation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
